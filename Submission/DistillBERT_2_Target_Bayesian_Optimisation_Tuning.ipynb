{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning of DistillBERT with Classification Head\n",
        "\n",
        "This notebook aims to improve on the limitation of the paper,\n",
        "https://doi.org/10.48550/arXiv.1905.05583, where the authors did not conduct hyperparameter tuning for their neural networks. For our project, we attempt to choose values for dropout rate and LEARNING_RATE variables used in 'DistillBERT_finetuning_2_target.ipynb' notebook, since learning rate affects convergence while dropout rate affects generalisation ability and train time of model. We used a subset of the training data, and utilized Bayesian Optimisation."
      ],
      "metadata": {
        "id": "1ysSqAbGQ0Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE2E6VdPx308",
        "outputId": "d1c7e706-3bcb-4922-bc91-6039c7b5d756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.4.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ3WdQcfsALW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d703a8a-a611-4076-ccc3-2c735e81e332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AdamW\n",
        "import re\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Import data and extract mini-set (500 rows) for hyperparametertuning. train_split was created with shuffling so no need shuffle again.\n",
        "drive.mount('/content/drive')\n",
        "train_split = pd.read_csv('/content/drive/MyDrive/train_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting subset of train data\n",
        "mini_set = train_split[0:500]\n",
        "val_set = train_split[500:551]\n",
        "\n",
        "# Convert to binary classification\n",
        "def good_bad(row):\n",
        "  if row < 5:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "mini_set['Sentiment'] = mini_set['Sentiment'].apply(good_bad)\n",
        "val_set['Sentiment'] = val_set['Sentiment'].apply(good_bad)"
      ],
      "metadata": {
        "id": "vS8gzZDmtIRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings in run logs\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Model and custom dataset classes similar to main notebook\n",
        "# Define Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    ''' Custom dataset class defined to create '''\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.Content = dataframe.Text.to_numpy()\n",
        "        self.targets = dataframe.Sentiment.to_numpy()\n",
        "        self.max_len = max_len\n",
        "\n",
        "    # __len__ and __getitem__ methods to create map-style dataset to be interfaced by torch DataLoader method\n",
        "    def __len__(self):\n",
        "        return len(self.Content)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Data preprocessing code to remove trailing whitespace, html tags and urls\n",
        "        Content = re.sub(r'<[^>]+>', '', self.Content[index])\n",
        "        Content = re.sub(r'https://\\S+|www\\.\\S+', '', Content)\n",
        "        Content = re.sub(r'br\\s', '', Content)\n",
        "        Content = \" \".join(Content.split())\n",
        "\n",
        "        rating = self.targets[index]\n",
        "\n",
        "        # Tokenisation of text\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            Content,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            pad_to_max_length=True,\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(rating, dtype=torch.int)\n",
        "        }\n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self, dropout_val):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(dropout_val)\n",
        "        self.classifier = torch.nn.Linear(768, 2)\n",
        "\n",
        "    # Note: DistilBERT outputs a tuple where the first element at index 0\n",
        "    # represents the hidden-state at the output of the model's last layer.\n",
        "    # It is a tensor of shape (batch_size, sequence_length, hidden_size=768)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "# Function to calcuate the accuracy of the model\n",
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "# Training Parameters\n",
        "MAX_LEN = 512\n",
        "EPOCHS = 5\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create validation set (fixed for all experiments)\n",
        "test_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'sampler': SequentialSampler(val_set),\n",
        "                'num_workers': 0\n",
        "                }\n",
        "val_data = CustomDataset(val_set, tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(val_data, **test_params)\n",
        "\n",
        "\n",
        "def train(lr_exponent_val, dropout_val):\n",
        "    # Ensure train_batch and lr_exponent_val are discrete\n",
        "    dropout = 0.1*int(dropout_val)\n",
        "    lr_exponent_val = int(lr_exponent_val)\n",
        "    lr = 1*10**-(lr_exponent_val)\n",
        "\n",
        "    # Create Dataset and Dataloader\n",
        "    paramtune_set = CustomDataset(mini_set, tokenizer, MAX_LEN)\n",
        "    train_params = {'batch_size': 4,\n",
        "                    'shuffle': True,\n",
        "                    'num_workers': 0\n",
        "                    }\n",
        "    paramtune_loader = DataLoader(paramtune_set, **train_params)\n",
        "\n",
        "    # Initialize model\n",
        "    model = DistillBERTClass(dropout)\n",
        "    model.to(device)\n",
        "\n",
        "    # Creating the loss function and optimizer\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop over mini_set\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for _,data in enumerate(paramtune_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(ids, mask)\n",
        "            loss = loss_function(outputs, targets)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluating model accuracy over test set\n",
        "    model.eval()\n",
        "    n_correct,nb_val_examples = 0,0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask)\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "            nb_val_examples+=targets.size(0)\n",
        "\n",
        "    run_accu = (n_correct*100)/nb_val_examples\n",
        "\n",
        "    return run_accu\n",
        "\n",
        "# Parameters to tune (learning rate and train batch size)\n",
        "pbounds = {\n",
        "    'dropout_val': (1,6),\n",
        "    'lr_exponent_val': (3, 7),\n",
        "    }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=train,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2,\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "# Bayesian Optimisation Algorithm. init_points parameter initiates 15 random points to explore during search. Helps by diversifying exploration space, increasing chances of finding global maxima.\n",
        "# n_iter specifies number of iterations of bayesian optimisation to run. Total iterations would be sum of n_iter and init_points.\n",
        "optimizer.maximize(init_points=15, n_iter=15)"
      ],
      "metadata": {
        "id": "3izVj0sKt-GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f2f0bf-cc9d-43f3-9d73-380069e67726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | dropou... | lr_exp... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m3.085    \u001b[0m | \u001b[0m5.881    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m1.001    \u001b[0m | \u001b[0m4.209    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m1.734    \u001b[0m | \u001b[0m3.369    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m1.931    \u001b[0m | \u001b[0m4.382    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m2.984    \u001b[0m | \u001b[0m5.155    \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m84.31    \u001b[0m | \u001b[95m3.096    \u001b[0m | \u001b[95m5.741    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m80.39    \u001b[0m | \u001b[0m2.022    \u001b[0m | \u001b[0m6.512    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m1.137    \u001b[0m | \u001b[0m5.682    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m80.39    \u001b[0m | \u001b[0m3.087    \u001b[0m | \u001b[0m5.235    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m1.702    \u001b[0m | \u001b[0m3.792    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m76.47    \u001b[0m | \u001b[0m5.004    \u001b[0m | \u001b[0m6.873    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m2.567    \u001b[0m | \u001b[0m5.769    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m84.31    \u001b[0m | \u001b[0m5.382    \u001b[0m | \u001b[0m6.578    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m43.14    \u001b[0m | \u001b[0m1.425    \u001b[0m | \u001b[0m3.156    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m78.43    \u001b[0m | \u001b[0m1.849    \u001b[0m | \u001b[0m6.513    \u001b[0m |\n",
            "| \u001b[0m16       \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m5.772    \u001b[0m |\n",
            "| \u001b[0m17       \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m4.977    \u001b[0m | \u001b[0m5.637    \u001b[0m |\n",
            "| \u001b[0m18       \u001b[0m | \u001b[0m84.31    \u001b[0m | \u001b[0m5.648    \u001b[0m | \u001b[0m4.499    \u001b[0m |\n",
            "| \u001b[0m19       \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m3.401    \u001b[0m |\n",
            "| \u001b[0m20       \u001b[0m | \u001b[0m80.39    \u001b[0m | \u001b[0m4.719    \u001b[0m | \u001b[0m4.545    \u001b[0m |\n",
            "| \u001b[0m21       \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m7.0      \u001b[0m |\n",
            "| \u001b[95m22       \u001b[0m | \u001b[95m86.27    \u001b[0m | \u001b[95m5.556    \u001b[0m | \u001b[95m5.133    \u001b[0m |\n",
            "| \u001b[0m23       \u001b[0m | \u001b[0m56.86    \u001b[0m | \u001b[0m3.645    \u001b[0m | \u001b[0m3.4      \u001b[0m |\n",
            "| \u001b[0m24       \u001b[0m | \u001b[0m45.1     \u001b[0m | \u001b[0m2.892    \u001b[0m | \u001b[0m7.0      \u001b[0m |\n",
            "| \u001b[95m25       \u001b[0m | \u001b[95m88.24    \u001b[0m | \u001b[95m4.119    \u001b[0m | \u001b[95m5.906    \u001b[0m |\n",
            "| \u001b[0m26       \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m4.191    \u001b[0m | \u001b[0m5.256    \u001b[0m |\n",
            "| \u001b[0m27       \u001b[0m | \u001b[0m82.35    \u001b[0m | \u001b[0m4.69     \u001b[0m | \u001b[0m6.253    \u001b[0m |\n",
            "| \u001b[0m28       \u001b[0m | \u001b[0m43.14    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.0      \u001b[0m |\n",
            "| \u001b[0m29       \u001b[0m | \u001b[0m86.27    \u001b[0m | \u001b[0m1.826    \u001b[0m | \u001b[0m5.677    \u001b[0m |\n",
            "| \u001b[0m30       \u001b[0m | \u001b[0m72.55    \u001b[0m | \u001b[0m5.999    \u001b[0m | \u001b[0m4.971    \u001b[0m |\n",
            "=================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Parameter numbers in table above are not the final parameters. Decimals are truncated and value is transformed to appropriate form (i.e. dropout_val of 3.085 means p=0.3 for dropout layer was used; lr_exponent_val = 5.881 means lr=1e-5 for AdamW optimiser was used.\n",
        "\n",
        "From the results above, we can see that evaluation accuracy is above 90% for trials with learning rate 1e-5. This experiment could not find an optimum value for dropout rate (top 3 iterations yielded different dropout rates)."
      ],
      "metadata": {
        "id": "D1deGVrKTbxt"
      }
    }
  ]
}